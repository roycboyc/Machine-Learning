This project was made as part of an Applied Machine Learning class with Serge Belongie

HW1:

1. Trained a k-nearest-neighbors algorithm to recognize sketches of digits and return identity of new digits in test set.
2. Trained a- algorithm on a list of Titanic passengers. Then, using demographics and type of cabin from the training dataset, the algorithm evaluated whether a hiterto-unseen list of passengers survived the Titanic disaster or not.

HW2:

1. Using the Yale Eigenfaces database, we calculated an “average” face, performed a Singular Value Decomposition, found the low-rank approximation, plotted the rank-r approximation error, represented the 2500-dimensional face image using an r-dimensional feature vector and used logistic regression to classify features from the top r vectors.

2. Using Kaggle’s “What’s Cooking” recipe database, we represented each dish by a binary ingredient feature vector, used Naive Bayes Classifier to predict cuisine using recipe ingredients, calculated the Naive Bayes’ accuracy assuming Gaussian and Bernoulli priors, used Logistic Regression to predict cuisine using recipe ingredients, and finally used our Bernoulli-prior Naive Bayes to compete in the Kaggle contest, where we reached 85% accuracy.

HW3:

1. 
